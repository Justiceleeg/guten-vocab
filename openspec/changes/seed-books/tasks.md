## 1. Book Seeding and Vocabulary Extraction

### 1.1 Create Book Selection Script (Phase 1)
- [x] Create `scripts/seed_books.py`
- [x] Load pgcorpus metadata CSV:
  - [x] Parse CSV file
  - [x] Handle encoding issues
  - [x] Extract relevant columns (gutenberg_id, title, author, category, language, download_count)
- [x] Filter books by category:
  - [x] Filter for "Children's Literature" OR "Children's Fiction" (also checks "subjects" column)
  - [x] Handle case variations
  - [x] Log filtering decisions
- [x] Filter by language:
  - [x] Filter for English only
  - [x] Handle language code variations
- [x] Filter by availability:
  - [x] Check if counts file exists for each book
  - [x] Skip books without counts files
- [ ] Calculate reading level:
  - [ ] Use textstat library (Flesch-Kincaid)
  - [ ] Load sample text from book (may need to download sample)
  - [ ] Calculate reading level score
  - [ ] Handle books where reading level cannot be calculated
  - **Note:** Currently skipped - requires book text which is not in counts files. Can be added later if needed.
- [ ] Filter to reading level 5.0-9.0:
  - [ ] Apply reading level filter
  - [ ] Log filtered books
  - **Note:** Skipped since reading level calculation is not implemented
- [x] Sort by download count (popularity):
  - [x] Sort descending by download_count
  - [x] Handle missing download counts
- [x] Select top 100 books:
  - [x] Take first 100 books after sorting
  - [x] Verify we have 100 books
- [x] Save selected books list:
  - [x] Create `data/books/` directory if needed
  - [x] Save to `/data/books/selected_books.json`
  - [x] Include: gutenberg_id, title, author, reading_level, download_count
  - [x] Format JSON nicely for readability

### 1.2 Extend Script for Vocabulary Extraction (Phase 2)
- [x] Load selected books from JSON (or use in-memory list)
- [x] For each selected book:
  - [x] Load pre-computed counts file from pgcorpus:
    - [x] Locate counts file for gutenberg_id (supports both Zenodo and pgcorpus formats)
    - [x] Parse counts file format (handles JSON and space/tab-separated)
    - [x] Handle missing or corrupted files
  - [x] Apply spaCy lemmatization:
    - [x] Load spaCy English model
    - [x] Lemmatize each word in counts (optimized with batch processing using `nlp.pipe()`)
    - [x] Create lemmatized word -> count mapping
  - [x] Filter to vocabulary_words table:
    - [x] Query vocabulary_words from database
    - [x] Match lemmatized words from book to vocabulary words
    - [x] Only keep words that exist in our vocabulary list
  - [x] Calculate total word count:
    - [x] Sum all word occurrences in book
    - [x] Store in book record
  - [x] Insert into `books` table:
    - [x] Use SQLAlchemy Book model
    - [x] Set title, author, gutenberg_id, reading_level, total_words
    - [x] Handle database errors
  - [x] Insert word counts into `book_vocabulary` table:
    - [x] For each matched vocabulary word
    - [x] Insert BookVocabulary record with occurrence_count
    - [x] Use batch processing for lemmatization (performance optimization)
    - [x] Handle foreign key constraints
- [x] Add progress tracking:
  - [x] Display "Processing book X of 100"
  - [x] Show current book title
  - [x] Show vocabulary matches found
- [x] Handle errors gracefully:
  - [x] Try-except around each book processing
  - [x] Log errors for failed books
  - [x] Continue processing remaining books
  - [x] Track which books failed and why
- [x] Print summary statistics:
  - [x] Total books processed successfully
  - [x] Total books failed
  - [x] Average vocabulary coverage per book
  - [x] Books with highest vocabulary coverage
  - [x] Books with lowest vocabulary coverage
  - [x] Total book_vocabulary records created

### 1.3 Test Book Seeding
- [x] Run `python scripts/seed_books.py`
- [x] Verify Phase 1 (book selection):
  - [x] Check `selected_books.json` was created
  - [x] Verify 100 books selected
  - [x] Verify all books have required fields
  - [ ] Check reading levels are in range 5.0-9.0 (skipped - reading level not calculated)
- [x] Verify Phase 2 (vocabulary extraction):
  - [x] Query: `SELECT COUNT(*) FROM books;` (100 books)
  - [x] Query: `SELECT COUNT(*) FROM book_vocabulary;` (10,914 relationships)
  - [x] Query top 10 books by vocab coverage
  - [x] Verify book_vocabulary records have correct foreign keys
  - [x] Check that vocabulary matches are correct
- [x] Verify error handling:
  - [x] Script continues if individual books fail
  - [x] Errors are logged appropriately
- [x] Add idempotency (can run multiple times safely)
  - [x] Script checks if book exists and updates if present
  - [x] Clears existing vocabulary before inserting new matches
  - [x] Safe to cancel mid-run (commits per book)

**Acceptance Criteria:**
- ✅ Book selection script filters and selects 100 appropriate books
- ✅ Selected books saved to `data/books/selected_books.json`
- ✅ All 100 books inserted into `books` table
- ✅ Book vocabulary counts extracted and inserted into `book_vocabulary` table
- ✅ Script handles errors gracefully
- ✅ Summary statistics printed
- ✅ Script is idempotent (can run multiple times)

